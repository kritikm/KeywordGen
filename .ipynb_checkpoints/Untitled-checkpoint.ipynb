{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation of RAKE - Rapid Automatic Keyword Extraction algorithm\n",
    "# as described in:\n",
    "# Rose, S., D. Engel, N. Cramer, and W. Cowley (2010). \n",
    "# Automatic keyword extraction from individual documents.\n",
    "# In M. W. Berry and J. Kogan (Eds.), Text Mining: Applications and Theory.unknown: John Wiley and Sons, Ltd.\n",
    "#\n",
    "# NOTE: The original code (from https://github.com/aneesha/RAKE)\n",
    "# has been extended by a_medelyan (zelandiya)\n",
    "# with a set of heuristics to decide whether a phrase is an acceptable candidate\n",
    "# as well as the ability to set frequency and phrase length parameters\n",
    "# important when dealing with longer documents\n",
    "#\n",
    "# NOTE 2: The code published by a_medelyan (https://github.com/zelandiya/RAKE-tutorial)\n",
    "# has been additionally extended by Marco Pegoraro to implement the adjoined candidate\n",
    "# feature described in section 1.2.3 of the original paper. Note that this creates the\n",
    "# need to modify the metric for the candidate score, because the adjoined candidates\n",
    "# have a very high score (because of the nature of the original score metric)\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import re\n",
    "import operator\n",
    "import six\n",
    "from six.moves import range\n",
    "from collections import Counter\n",
    "\n",
    "debug = False\n",
    "test = True\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s) if '.' in s else int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_stop_words(stop_word_file):\n",
    "    \"\"\"\n",
    "    Utility function to load stop words from a file and return as a list of words\n",
    "    @param stop_word_file Path and file name of a file containing stop words.\n",
    "    @return list A list of stop words.\n",
    "    \"\"\"\n",
    "    stop_words = []\n",
    "    for line in open(stop_word_file):\n",
    "        if line.strip()[0:1] != \"#\":\n",
    "            for word in line.split():  # in case more than one per line\n",
    "                stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def separate_words(text, min_word_return_size):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of all words that are have a length greater than a specified number of characters.\n",
    "    @param text The text that must be split in to words.\n",
    "    @param min_word_return_size The minimum no of characters a word must have to be included.\n",
    "    \"\"\"\n",
    "    splitter = re.compile('\\b[^\\s]+\\b')\n",
    "    words = []\n",
    "    for single_word in splitter.split(text):\n",
    "        current_word = single_word.strip().lower()\n",
    "        # leave numbers in phrase, but don't count as words, since they tend to invalidate scores of their phrases\n",
    "        if len(current_word) > min_word_return_size and current_word != '' and not is_number(current_word):\n",
    "            words.append(current_word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of sentences.\n",
    "    @param text The text that must be split in to sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def build_stop_word_regex(stop_word_list):\n",
    "    stop_word_regex_list = []\n",
    "    for word in stop_word_list:\n",
    "        word_regex = '\\\\b' + word + '\\\\b'\n",
    "        stop_word_regex_list.append(word_regex)\n",
    "    stop_word_pattern = re.compile('|'.join(stop_word_regex_list), re.IGNORECASE)\n",
    "    return stop_word_pattern\n",
    "\n",
    "\n",
    "#\n",
    "# Function that extracts the adjoined candidates from a list of sentences and filters them by frequency\n",
    "#\n",
    "def extract_adjoined_candidates(sentence_list, stoplist, min_keywords, max_keywords, min_freq):\n",
    "    adjoined_candidates = []\n",
    "    for s in sentence_list:\n",
    "        # Extracts the candidates from each single sentence and adds them to the list\n",
    "        adjoined_candidates += adjoined_candidates_from_sentence(s, stoplist, min_keywords, max_keywords)\n",
    "    # Filters the candidates and returns them\n",
    "    return filter_adjoined_candidates(adjoined_candidates, min_freq)\n",
    "\n",
    "\n",
    "# return adjoined_candidates\n",
    "\n",
    "#\n",
    "# Function that extracts the adjoined candidates from a single sentence\n",
    "#\n",
    "def adjoined_candidates_from_sentence(s, stoplist, min_keywords, max_keywords):\n",
    "    # Initializes the candidate list to empty\n",
    "    candidates = []\n",
    "    # Splits the sentence to get a list of lowercase words\n",
    "    sl = s.lower().split()\n",
    "    # For each possible length of the adjoined candidate\n",
    "    for num_keywords in range(min_keywords, max_keywords + 1):\n",
    "        # Until the third-last word\n",
    "        for i in range(0, len(sl) - num_keywords):\n",
    "            # Position i marks the first word of the candidate. Proceeds only if it's not a stopword\n",
    "            if sl[i] not in stoplist:\n",
    "                candidate = sl[i]\n",
    "                # Initializes j (the pointer to the next word) to 1\n",
    "                j = 1\n",
    "                # Initializes the word counter. This counts the non-stopwords words in the candidate\n",
    "                keyword_counter = 1\n",
    "                contains_stopword = False\n",
    "                # Until the word count reaches the maximum number of keywords or the end is reached\n",
    "                while keyword_counter < num_keywords and i + j < len(sl):\n",
    "                    # Adds the next word to the candidate\n",
    "                    candidate = candidate + ' ' + sl[i + j]\n",
    "                    # If it's not a stopword, increase the word counter. If it is, turn on the flag\n",
    "                    if sl[i + j] not in stoplist:\n",
    "                        keyword_counter += 1\n",
    "                    else:\n",
    "                        contains_stopword = True\n",
    "                    # Next position\n",
    "                    j += 1\n",
    "                # Adds the candidate to the list only if:\n",
    "                # 1) it contains at least a stopword (if it doesn't it's already been considered)\n",
    "                # AND\n",
    "                # 2) the last word is not a stopword\n",
    "                # AND\n",
    "                # 3) the adjoined candidate keyphrase contains exactly the correct number of keywords (to avoid doubles)\n",
    "                if contains_stopword and candidate.split()[-1] not in stoplist and keyword_counter == num_keywords:\n",
    "                    candidates.append(candidate)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "#\n",
    "# Function that filters the adjoined candidates to keep only those that appears with a certain frequency\n",
    "#\n",
    "def filter_adjoined_candidates(candidates, min_freq):\n",
    "    # Creates a dictionary where the key is the candidate and the value is the frequency of the candidate\n",
    "    candidates_freq = Counter(candidates)\n",
    "    filtered_candidates = []\n",
    "    # Uses the dictionary to filter the candidates\n",
    "    for candidate in candidates:\n",
    "        freq = candidates_freq[candidate]\n",
    "        if freq >= min_freq:\n",
    "            filtered_candidates.append(candidate)\n",
    "    return filtered_candidates\n",
    "\n",
    "\n",
    "def generate_candidate_keywords(sentence_list, stopword_pattern, stop_word_list, min_char_length=1, max_words_length=5,\n",
    "                                min_words_length_adj=1, max_words_length_adj=1, min_phrase_freq_adj=2):\n",
    "    phrase_list = []\n",
    "    for s in sentence_list:\n",
    "        tmp = re.sub(stopword_pattern, '|', s.strip())\n",
    "        phrases = tmp.split(\"|\")\n",
    "        for phrase in phrases:\n",
    "            phrase = phrase.strip().lower()\n",
    "            if phrase != \"\" and is_acceptable(phrase, min_char_length, max_words_length):\n",
    "                phrase_list.append(phrase)\n",
    "    phrase_list += extract_adjoined_candidates(sentence_list, stop_word_list, min_words_length_adj,\n",
    "                                               max_words_length_adj, min_phrase_freq_adj)\n",
    "    return phrase_list\n",
    "\n",
    "\n",
    "def is_acceptable(phrase, min_char_length, max_words_length):\n",
    "    # a phrase must have a min length in characters\n",
    "    if len(phrase) < min_char_length:\n",
    "        return 0\n",
    "\n",
    "    # a phrase must have a max number of words\n",
    "    words = phrase.split()\n",
    "    if len(words) > max_words_length:\n",
    "        return 0\n",
    "\n",
    "    digits = 0\n",
    "    alpha = 0\n",
    "    for i in range(0, len(phrase)):\n",
    "        if phrase[i].isdigit():\n",
    "            digits += 1\n",
    "        elif phrase[i].isalpha():\n",
    "            alpha += 1\n",
    "\n",
    "    # a phrase must have at least one alpha character\n",
    "    if alpha == 0:\n",
    "        return 0\n",
    "\n",
    "    # a phrase must have more alpha than digits characters\n",
    "    if digits > alpha:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def calculate_word_scores(phraseList):\n",
    "    word_frequency = {}\n",
    "    word_degree = {}\n",
    "    for phrase in phraseList:\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        word_list_length = len(word_list)\n",
    "        word_list_degree = word_list_length - 1\n",
    "        # if word_list_degree > 3: word_list_degree = 3 #exp.\n",
    "        for word in word_list:\n",
    "            word_frequency.setdefault(word, 0)\n",
    "            word_frequency[word] += 1\n",
    "            word_degree.setdefault(word, 0)\n",
    "            word_degree[word] += word_list_degree  # orig.\n",
    "            # word_degree[word] += 1/(word_list_length*1.0) #exp.\n",
    "    for item in word_frequency:\n",
    "        word_degree[item] = word_degree[item] + word_frequency[item]\n",
    "\n",
    "    # Calculate Word scores = deg(w)/frew(w)\n",
    "    word_score = {}\n",
    "    for item in word_frequency:\n",
    "        word_score.setdefault(item, 0)\n",
    "        word_score[item] = word_degree[item] / (word_frequency[item] * 1.0)  # orig.\n",
    "    # word_score[item] = word_frequency[item]/(word_degree[item] * 1.0) #exp.\n",
    "    return word_score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phrase_list, word_score, min_keyword_frequency=1):\n",
    "    keyword_candidates = {}\n",
    "    for phrase in phrase_list:\n",
    "        if min_keyword_frequency > 1:\n",
    "            if phrase_list.count(phrase) < min_keyword_frequency:\n",
    "                continue\n",
    "        keyword_candidates.setdefault(phrase, 0)\n",
    "        word_list = separate_words(phrase, 0)\n",
    "        candidate_score = 0\n",
    "        for word in word_list:\n",
    "            candidate_score += word_score[word]\n",
    "        keyword_candidates[phrase] = candidate_score\n",
    "    return keyword_candidates\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "    def __init__(self, stop_words_path, min_char_length=1, max_words_length=5, min_keyword_frequency=1,\n",
    "                 min_words_length_adj=1, max_words_length_adj=1, min_phrase_freq_adj=2):\n",
    "        self.__stop_words_path = stop_words_path\n",
    "        self.__stop_words_list = load_stop_words(stop_words_path)\n",
    "        self.__min_char_length = min_char_length\n",
    "        self.__max_words_length = max_words_length\n",
    "        self.__min_keyword_frequency = min_keyword_frequency\n",
    "        self.__min_words_length_adj = min_words_length_adj\n",
    "        self.__max_words_length_adj = max_words_length_adj\n",
    "        self.__min_phrase_freq_adj = min_phrase_freq_adj\n",
    "\n",
    "    def run(self, text):\n",
    "        sentence_list = split_sentences(text)\n",
    "\n",
    "        stop_words_pattern = build_stop_word_regex(self.__stop_words_list)\n",
    "\n",
    "        phrase_list = generate_candidate_keywords(sentence_list, stop_words_pattern, self.__stop_words_list,\n",
    "                                                  self.__min_char_length, self.__max_words_length,\n",
    "                                                  self.__min_words_length_adj, self.__max_words_length_adj,\n",
    "                                                  self.__min_phrase_freq_adj)\n",
    "\n",
    "        word_scores = calculate_word_scores(phrase_list)\n",
    "\n",
    "        keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores, self.__min_keyword_frequency)\n",
    "\n",
    "        sorted_keywords = sorted(six.iteritems(keyword_candidates), key=operator.itemgetter(1), reverse=True)\n",
    "        return sorted_keywords\n",
    "\n",
    "# if test:\n",
    "# #     text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types.\"\n",
    "#     stopwordpattern = build_stop_word_regex(stoppath)\n",
    "\n",
    "#     # generate candidate keywords\n",
    "#     phraseList = generate_candidate_keywords(sentenceList, stopwordpattern, load_stop_words(stoppath))\n",
    "\n",
    "#     # calculate individual word scores\n",
    "#     wordscores = calculate_word_scores(phraseList)\n",
    "    \n",
    "# #     for word in wordscores:\n",
    "# #         print(\"%s: %f\" %(word, wordscores[word]))\n",
    "#     # generate candidate keyword scores\n",
    "# #     keywordcandidates = generate_candidate_keyword_scores(phraseList, wordscores)\n",
    "# #     if debug: print(keywordcandidates)\n",
    "\n",
    "# #     sortedKeywords = sorted(six.iteritems(keywordcandidates), key=operator.itemgetter(1), reverse=True)\n",
    "# #     if debug: print(sortedKeywords)\n",
    "\n",
    "# #     totalKeywords = len(sortedKeywords)\n",
    "# #     if debug: print(totalKeywords)\n",
    "# #     for key in sortedKeywords[0:(totalKeywords // 3)]:\n",
    "# #         print(key)\n",
    "# # #     print(sortedKeywords[0:(totalKeywords // 3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "projectDir = '/home/rkb/Documents/KeywordGen/'\n",
    "songsDir = projectDir + 'songs'\n",
    "stopwordsPath = projectDir + 'stopwords'\n",
    "rake_keys = projectDir + 'rake_keys/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = os.listdir(songsDir)\n",
    "\n",
    "for name in names:\n",
    "    filePath = os.path.join(songsDir, name)\n",
    "    if os.path.isfile(filePath):\n",
    "        with open(filePath) as f:\n",
    "            print(\"Extracting keywords from file %s\" %name)\n",
    "            text = f.read()\n",
    "#             sentenceList = split_sentences(text)\n",
    "            stoppath = \"stopwords\"\n",
    "            rake = Rake(stoppath)\n",
    "            keywords = rake.run(text)\n",
    "            keywordFile = os.path.join(rake_keys, name + '-keys')\n",
    "            with open(keywordFile)\n",
    "            for key in keywords:\n",
    "                print(key)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"songs/32\") as f:\n",
    "    text = f.read()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
